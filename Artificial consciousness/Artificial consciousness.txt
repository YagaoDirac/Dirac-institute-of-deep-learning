Artificial consciousness

yagaodirac
twitter:yagaodirac
github.com/yagaodirac/dirac-institute-of-deep-learning

2023 apr 15

The human nervous system can be roughly approximated as a diffusion (the derivation process will be added later). If this diffusion runs infinite steps, is it a living consciousness?
As pic1 shows, it looks like some resnet or rnn.

In order to let multiple diffusions to exchange informations, the output(also the input) for each diffusion network should share some parts. It looks like they use the same part of a common memory, or they overlap a bit. Such structures can be in any shapes, they can simply overlap, or directly send something to some specific target, or cross-send information to each other. With these fancy structures, it's possible to isolate some parts from others, also the amount of parameters should be reduced a lot while the performance may maintain well enough. The actual effect of using such fancy structures may be similar to that between different structures of the brain or the soft isolation in the spine.

The overlap and cross structures are shown in pic1.png.

Then, some extra topics.
The system only needs to maintain certain output values in a range. When these values ​​are outside this range(due to normal updates), some training is needed. The neural nets will be trained to keep these special numbers back to some given range (or any equivalent method). After training, we can either rollback(using some old good output directly but with new weights and bias), or continue to run after training, and wait for the neural nets itself to go back to the scope of the design. The whole neural nets looks like only to maintain these values. I call this bionics.
This looks a bit wierd. But it's pretty useful imo.
If some things are linked to these values that must be within a certain range, will the entire neural nets eventually show a tendency, similar to interest. Imo, we can build intrisic characteristics for the artificial consciousness.
This process is shown in pic2.png.

With only diffusion structure, imo it's only possible to achieve abstraction, but not possible to achieve reasoning. 
Abstraction is only about, when some condition is meeted, then a corresponding result will show with a really big possibility. Let's say in step n, something happens, then with a 90%
of probability in step n+m, something else happens. If the neural net is trained with some earlier result as input and some later result as label, it may learn from itself. It feels like the model distills itself. This can help the model get the same result in less steps.
The training result looks like abstraction or correlationship, but it's never mathematically reasoning.

<update 2023 apr 19>
Is curiosity a stand-alone concept? When I tried to figure out how this infinite steps diffusion model can achieve curiosity, I found it out from myself what I care is if something is useful. It's not about atractive by something new, it's by something useful. I'm chasing the productiveness. If curiosity is only the superficial, but values is the intrinsic, then it should be achieved by training not by structure or hard code from outside.
</update 2023 apr 19>

So, what is unsolved so far.
Although diffusion models work super well in image generation, but in my design, I don't know how to make this system to do any generation, especially when effection should be also considered.
If this model can only distill itself, it only builds shortcuts, it's not possible to really make anything really new, unless innovation can come from scaling.

The last thing, where is this model used.
The design is only about a inner latent representation, and how it's maintained, updated, used. When we humans want to interact with it, we still need language models to help translate our meaning to the latent representation this model understands, and then the final result translated into natural language so we understand.







