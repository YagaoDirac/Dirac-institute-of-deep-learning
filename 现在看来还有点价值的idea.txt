to do list:


以下是以前的。门层的地方参考上面的内容。








【【【对抗梯度消失】】】

gonnor or gonnaa, Gonnor?, Gradient ONly NormAlizAtion
注意要把learning rate乘上去。然后需要在外面的容器里面写一个更新。
grad里面是 g = normalize(g)*learning_rate。
如果和mirror一起用，看后面。



【【【mirror层】】】

上面说了Gonnor，这儿说一下mirror。
这个设计主要是针对输入的x过于接近0的时候，直接和 x 相乘的参数会无法得到更新。
这个事情是无法避免的，不光是开头，也包括所有的中间隐藏层，一定会遇到。
方法如下，以全连接层为例
本来的公式应该是 out = w*x+b，学习率是lr
设置为训练模式的时候，设置 w1 = w2 = w，half_lr = lr/2，之后， out = （ w1 * ( x + half_lr ) + w2 * ( x - half_lr )）/2 + b
x+lr和x-lr对格子对应的w带来的更新强度加起来至少等于lr，
每经过大概n次的更新w1和w2之后，
w = （w1+w2）/2
之后再设置成训练模式，从而让w1 = w2。
预测的时候，还是只使用一个w。



【【【gonnor和mirror一起用的时候的注意事项】】】

训练的时候的正向传播是 out = （ w1 * ( x + half_lr ) + w2 * ( x - half_lr )）/2 + b
这个正向传播自带了一个half_lr*2的学习率。如果同时给gonnor设置不等于1的学习率，就会出问题。
grad里面是 g = normalize(g)，注意，不乘以learning_rate。
也就是说mirror是作为一个整体，只在一个地方进行一次gonnor。
同样的，因为要从外面获取learning rate，所以需要一些额外的代码。




【【【关于对叠层的可训练性的一些其他的间接证据】】】

以及为什么我喜欢gonnor

如果这个课训练性可以纯粹用反向传播的梯度的长度来直接的衡量的话，那么，可以得到的证据至少有，
alexnet初始化的时候，有一半的层的mean是1，另外一半是0，而且是交叉出现的。
positional encoding位置编码
利用sin和gaussian做激活的nerf，也就是siren和garf
transformer里面的softmax
以及直接看那些权重就知道了。
这个事情直接推出3个结论，gbn，mirror，和如果要实现gate，那么应该用sigmoid替代if，也是很常规的做法了。




【【【没想清楚的系列】】】

diffusion用相同的网络对中间结果进行迭代。res net用不同的层对相似的activation进行变化。能不能结合一下？
比如2个diffusion nn，噪音进去，先用1，然后用2，然后再1，再2。
简写就是，1212，或者1122，或者1111222211112222，或者更多。这样是否有可能准备一堆各种不同的网络，然后对同一个东西进行不同的处理？比如
111222，然后发现3好像比较适合，然后333，然后突然觉得好像1又不错，结果就是111222333111，这样。









【【【门层】】】

基本有解了。
2个事情，
非二值化参数拿进来要先二值化。
x = tanh(x*100)
可以100，也可以什么别的。
这儿可能会出一些问题，所以可能应该是采用
x = tanh(x*3)
x = tanh(x*3)
用2次。但是可能一次的就已经很ok了。

二值化之后的参数就可以正常用了。剩下就和数字电路基本一样了
之后就是数字电路那一套了，随便整了。
And就是
and2 = tanh((x1+x2+0.333333)*100)
假设x1和2都是tanh出来的，所以是+-1。偏移可能0.333合适，也可能0.5合适。
其他层都类似。
xor可以用乘法，利用符号来解决。
and3可以考虑 tanh((x1+x2+x3+0.75)*100)一类的。还是老问题，注意偏移，要实测一下。

这个东西可能可以让神经网络解决操作整数的问题，然后可以操作整数了，去跟程序员问问你暂时缺乏的想象力。

==========后面这一段是之前写的，思路不成熟，可以跳过。==========
自己和自己and，or，之后cat一起，不能用true false，要用sigmoid一类的某一个东西，加一些数学上的计算。之后需要用一个白层来变形。加一个细节，第一个门层之前要加一个没有激活的全连接层，不然第一个门层会出问题。输入进来以后，x = concat(x, -x)，从而得到输入的非，这一步可能没什么帮助。到这个地方，x的取值范围理论上是任意的。之后，head_and = sigmoid(x)+sigmoid(xt)-1，或者"(sigmoid(x)+sigmoid(xt))按位乘以(sigmoid(x)+sigmoid(xt))"，或者用pow函数，我不清楚性能，pow好像比直接乘要慢一点。维度也会有一些变化，如果x是一个n维的vector，那么它就是1*n，xt是x的转置，是n*1，两个按位相乘之前必须要扩展成n*n，不过现在的不管是tf，pytorch，好像都有默认的广播机制了，所以我就没强调，相乘以后会得到n*n的矩阵，head_and就是一个n*n维的matrix，其他的头的维度也类似。head_or = sigmoid(x)+sigmoid(xt),或者, "(1-(sigmoid(x)+sigmoid(xt)))按位乘以(1-(sigmoid(x)+sigmoid(xt)))"，head_xor = (( (sigmoid(x)*2-1)按位乘以(sigmoid(xt)*2-1) )+1)/2，可以不需要最后的+1)/2，后面的部分可以补偿这个细节，其他的可能的公式有 sigmoid(x按位乘以xt),或者在这个公式里面，用ln(x)替代x，并且保留符号，比如x<0的时候是-ln(x)，从而避免数字太大。
至于需要不需要每一个层的非版本，我觉得可能不用。甚至xor层到底有没有必要都还不一定。
之后，x = concat(x, head_***, head_****, head_*****, ...)
x = w*x+b  //不要加激活函数，只是为了变形。
稍微汇总一下：
x = concat(x, -x)//optional
head_*** = ...
head_**** = ...
head_***** = ...
x = concat(x, head_***, head_****, head_*****, ...)
x = w*x+b
return x
这个门层是用来替代transformer层的。我的观点是，全连接层可以得到或关系，但是现在只有transformer层里面才多少有一些且关系，所以它的性能好。或者说，有可能可以用全连接层来替代head_or。至于需要哪些，比如是否需要-x，是否需要xor，都是可选的，有待验证。
这个门层应该是不兼容bn的，可能需要mirror，应该是兼容gbn的。
这个层的缺点，
1是，不太好手写一些固定的数学公式，如果需要手写数学公式可能还是传统的全连接层比较容易。
2，参数量太大。整个层唯一的参数就是x = w*x+b里面的w和b，其他w的尺寸 = n*n*n*(count of heads)，如果在开头引入了非，那么n还要乘以2。解决思路可能要从2019年的wann里面去找，因为我觉得w里面的大多数地方都会是0。
这个层可能比较擅长的还是在语言模型一类的，不用很线性的，地方。
可能可以使用bf8一类的数据结构，毕竟不用很好的精度。但是如果真的用很小的数据结构的时候要注意数字的可表达范围，需要额外的保护。

关于门层，2023年7月，最新想到的，可以用一个double relu 来做。
h1 = relu(x)
h2 = relu(-x)
x = concat(h1, h2)
return x
会变宽。不知道效果如何。变化的话，可能，中间稍微留一小段的0，然后两边也用一个x**2，或者x**1.2一类的，不要用直线，可能会稍微好点。比如
h1 = relu(x-1)**2
h2 = relu(-x-1)**2
x = concat(h1, h2)
return x

其实一层dense relu可以做除了xor之外的4个门，剩xor和另外一个，可能需要额外一层，没仔细验证。
想了一下，可能要尽量多个相互独立的单元，但是这些相互独立的单元又不能真的用相互独立的层，不然显卡上可能会很难看。所以，是不是类似conv 1*n,window size 用 2，步长也是2，这样就刚好隔离开，甚至可以跨层，也是隔离的。
理由是，最近看到包括yann也在说，当层的宽度达到一个很夸张的值的时候，似乎有一些神奇的事情发生了。我的解释就是逻辑之间更容易相互隔离的，毕竟反向传播这种模模糊糊的东西，相互干扰起来简直毫无人性，根本就是不可控的。如果我的解释是足够准确的，那么网络结构上形成隔离应该可以提升性能。







dense层按我的理解只能做或关系，所以如果可以让dense的W按照每一个输出float一组，组内做softmax，这个softmax是做到W的参数上的，不是x，不是激活。是不是就可以得到一个类似且关系的东西，从而替代transformer？
另外，wann，应该也是几年前的文章，我觉得那个做法肯定是有说法的，只是需要把遗传算法改成可反向传播的。
transformer的那个自注意力的结构，按我的说法是可以得到且关系，但是为什么没有听说过这个解释？

关于激活函数。
一个基本思路是，太平的地方没有梯度，太陡峭可能也不行，梯度会很大。输出值太大的也不行。覆盖的输入值太少的也不行。这样看来就只能是周期函数了？还必须是连续的，所以不能是锯齿，至少不能突变，就是双锯齿可以，单锯齿不行。正弦一类的。如果可以在正弦的两边加上sigmoid那种可以锁的极端值的设计，是不是可以做出一个又能训练，又能直接用的东西？

现在有一个比较严重的问题。其实无论是现在的transformer层，还是我提出的任何一个实现了原生的乘法或者且关系的层，都无法解决一个问题，就是中间表达的相互污染的问题。比如在一个dense里面，要得到2个且关系，就必须是各自在各自的地方，非，或，非（ 因为(a and b) == not(not(a) or not(b)) ），这个还没考虑激活函数是如何表达负逻辑的，假设激活函数可以正常的表达负逻辑，那么这需要至少2个dense层，两个dense的中间会出现一个连接关系，这个地方就会导致互相的污染。也就是说，要么我们用汤的方式来做，或者叫做，用2个dense网络，最后综合考虑（concat，或者相加，或者其他什么方法）。如果要用一个整的dense，自动调节来得到事实上相互分立的区域，从而实现一个最大的灵活性，那么最终的网络里面应该多数地方都是0。现在的权重的更新的方法很难让这些权重真的回到一个很干净的0，所以可能需要考虑一下额外的措施。如果真的可以把大多数地方都更新到0，那么应该有可能把大的神经网络拆成很多小的，并且保持数学上的等价。这个做法好像和resnet不兼容。。











【【【用数学符号来表示数据的内在规律】】】

或者叫，破解数学公式。

另外一个是，破解数学公式，其实就是要在神经网络的中间过程保护每一个层的输出，并且这个保护动作要同步的作用到label上。我想到的办法是，x = concat(input, label),之后每一层输出都用一个bn或者softmax一类的东西来保护一下，这个保护会同步的作用到label上，然后到最后，output, new_label = spli(x,(1,1))，也就是单独把label拆出来，这个时候的x也是标量了，分别用变量output和new_label保存，之后，x = output - new_label，完美的情况下，x应该等于0，有任何的误差的话都不是0，然后看情况要不要加一句x = x*x，来保证x一定大于等于0，之后手动配的label永远是0。代码上来说，fit的地方，x是concat的结果，y是0。
完事以后，考虑一下蒸馏，然后手动处理符号层面的变换。需要什么运算，就做一个什么运算对应的层，应该就行了。

=========以前的笔记，暂时没整理。=========
之前看到有人在做从输入输出来反过来猜符号化的公式。
第一个方法好像数学上不成立。不过如果引入不同的运算的层会对结果有帮助的话，可能可以在别的地方用用。
dense只能处理线性，加上激活函数，能用的内容也很少。不过也不算很少了，只是不太能和实际的公式对应。
我的思路是，dense可以处理线性，那么所有的加减就有了。剩下的就是乘法，除法，乘方，对数，分别做一个对应的层，然后对同一个输入，对每一个层做一次，然后concat起来。这个会导致数据变宽很多，我的想法是，用一个dense relu再改回一个足够窄的宽度，比如20~50这种，同时relu可以提供一个很清晰的判断逻辑。
这个做法的一个核心问题是，每一层的输出都可能会很大，然后接下来的一层的乘方里面就会出现inf。解决方法是在每一层的输出上做调整，做一次normalization，但是问题是，这个normalization会影响最终的结果，所以需要记录所有的normalization对结果的影响。那么问题就来了，对不同的运算层，这个normalization造成的缩放，对后续的结论的影响是不同的。或者可能可以额外concat一个常数，或者几个常数，然后一起扔进一个softmax。当然这样一来，label也必须要做同样的操作。softmax可能会对反向传播有一些影响，不过不好说。不过softmax最大的问题是可能会对符号化的公式产生影响，因为牵扯到迭代。就算最后手工修正，也可能会超级麻烦。
另外一个办法是，做一个sigmoid，至少所有的东西都会在0~1之间，然后再进入后面的层，至少每一层的输入都会在一个比较安全的范围内。
最后一个细节，到底要堆几层？
第二个方法比较那啥，我没有这一块的直觉，不知道在数学上成立不成立。
比如有一个输入和一个输出，分别是x和y。当然也可以有多个输入。
用2个dense 64 relu，或者3层，这样，然后最后一层dense 1，全部有bias。（pytorch里面，dense叫做linear）。输入是x，输出是y，训练，之后对每一层的权重进行排序，排序的依据嘛，我还没想好，比如W[i]的长度，或者abs的平均值，一类的，反正要排序。然后W[i+1]-W[i]，有点类似微分，或者这一步也可以换成某种别的什么。然后那这个类似微分的权重值，作为输入，去另外一个神经网络，然后猜，这里面的一个成分。比如另外一个神经网络猜可能有x*x，那么接下来，用（x, x*x)作为输入，y作为输出，重新训练“2个dense 64 relu”的神经网络，然后权重排序，做类似微分的运算，送到“另外一个神经网络”，猜其他的成分。
另外一个神经网络可以用任何的back bone，他要做的事情就是猜还有什么其他的成分。输入格式就是前面说的，输出格式大概是，第n个输入和第n个输入做一个什么运算，作为新的输入元素。
当然如果训练到一个什么时候，发现某一个什么输入元素对应的权重全部都是0了，就可以给它删了，可能是第二个神经网络的某种中间结果。
灵感来源就是最早的有什么deepmind还是谁，有一个playground，里面就是同时利用了x,y,x*x, x*y, y*y, sin(x), sin(y)，可能还有一些别的，做输入。这样就直接的弥补了dense层无法让x[i]和x[j]相乘，无论i和j相等或者不相等，如果要强行用dense拟合一个x[i]*x[j]，根据我的经验，这一个乘法会污染整个dense层，无论有多宽。

反正2个方法都是无监督学习，数据是无限的。个人觉得第二个可能还靠谱点。






可能WANN和上面的数学公式的部分结合一下，会不错？
WANN和一个相关的研究，神经网络的自行生长 https://arxiv.org/abs/2307.08197
我没仔细读。WANN是用的遗传算法来解决不连续表达的问题，但是很明显，遗传算法不是一个能带来想象力的东西。
我后来想了一下，可能真正重要的是生长的依据。
如果假设是dense层，那么如果一个w，或者一个b，积累的更新方向是相互矛盾的，那么可能在暗示现在的这种wx+b的关系是不对的，这种就可以考虑生长一下。这是我的理解，没验证过。但是生长出来的东西应该是什么样，我觉得是不是可以先长一个全的，然后再删？
那么删的依据，我觉得如果权重的绝对值足够小，应该就是无关的了，应该就可以删了。
但是这样做的话，很数学，很不并行计算，可能可以找到一个靠谱的网络结构，但是可能最后只能是扔回cpu？








序列长度自适应，搭配diffusion。(注意，每一个token背后是一个向量，在这儿用一个单独的字母表示）
输入文本abc，重复一下，得到aabbcc，如果最后有一个结束符，也类似。甚至在开头还可以加一个开始。
让nn去学aabbcc，可以用类似窗口尺寸=3的卷积一类的，学习的核心结构是diffusion。
最后生成的时候，扔固定长度的噪音进去，比如xyz（可以不是xxyyzz，可以纯粹是完全的噪音），然后让nn自己去得到aabbcc作为输出。
然后就是长度自适应的技巧。
如果从aabb，变成aacb，那么手动把新出现的和原来的变少了的重复一下，变成aaccbb。如果是acbb，也是类似，就是aaccbb。
如果是aabbcc，变成了aaabcc，那么把超过2个长度的，还原到长度2的重复，自己变短的就不管了。那么aaabcc会变成aabcc。这个情况和前面的情况的差异是，第一个b的前面是a，所以当这个b变成a的时候，意味着这个序列应该要变短。这儿也可以直接把aaabcc直接变成aacc。
这个结构无法处理原始文本里面就有重复的情况，这种情况要用一个小技巧，比如，aab，先填一个占位符在两个a中间，比如变成a_ab，然后再重复，占位符可以重复可以不重复，得到aa__aabb或者aa_aabb。
这个事情不干扰词嵌入。
还有一个有可能的用途，就是图片的扭曲，可以认为是图片的一个局部的尺寸要变大或者变小。
应该来说还有别的更扯的应用，就不展开了。


我记得2020年还有一个wann，那个思路也是很猛的，我觉得很有前途的，可惜他们用的是遗传算法。


如果允许神经网络去调用别的工具，甚至别的神经网络，会如何。现在好像语言模型后面有人在尝试加这个了。这个后续会引出一个问题，就是有没有什么方法可以让一个神经网络去自己决定要怎么训练它自己，因为一个固定的神经网络去调用外部的一切东西，包括调用其他的nn，训练其他的nn，都好理解，但是它要怎么决定怎么训练它自己？会是强化学习吗？




曾经测试过的，w*(pow(x, a)) +b，a在-5到3之间好像效果还不错，但是如果用2，4这种数字，会有一个无法在实数里面用的问题。当然b也可以不是一个单纯的b，也可以是某个参数的n次方。有可能学习速度会增加。



如果改进视频生成的连贯性。
利用延迟渲染的思路，中间结果经过光照模型，得到最终的rgb，甚至还有a。如果可以利用神经网络返回去，应该可以得到那些中间结果，然后在中间结果上做渐变，然后再利用光照模型计算最终的rgb。光照模型是写死的算法，所以只有前两步是需要神经网络的。至于数据集，不知道。



类似最近的lola，lora。peft，parameter effetient fine turning，就是在原有的结构上加入新的层来做微调，原来的层全部锁住。我看到的情况，基本都是在有残差连接的结构里面做，而且都是不跨残差的，那么在同一个残差连接里面，相当于有更多的层。那是不是可以认为，原来的残差连接连得太近了？是否有可能对比一下2层一连，3层一连，4层一连，6层一连，10层一连，这种？


各种生成数据集的方法。
直接利用原来统计上的聚类算法，得到一个准确率可能不一定怎么样的一个数据集，然后用一个偏小的nn去学，有可能会扛掉噪音，从而提高准确性。
可以用图片半透明的方法做物体个数的数据生成，然后不知道这个有用没。

（这一段是纯粹的草稿，可以不管）
如果是直接用一个初始化到随机的nn，去给一堆样本分类，然后利用它自己得到的分类，来训练它自己，会不会提升分类的精度？也就是说，如果利用一个比较小的nn，就是直接小到根本不可能过拟合，利用这种欠拟合的能力去扛噪音，包括数据集本身的噪音，包括它自己预测的时候的各种错，是否有可能。然后逐渐的换更大一点的nn。整个过程不需要任何的label，如果可行的话。
这个过程里面，要按什么来做标准呢，比如，样本是5个类别，而且不是数量相等的5个类别，刚开始应该分到5个类，还是2个类呢？如果分到2个类，那么相似的就会分到一起，然后在这个范围内再分吗。如果直接分5个类，如果对应不上，是不是也无法保证5个类都是有效的。如果更多的类，比如1000类，这种，要怎么办，从2类开始一点一点的人工辅助着来吗？这个做法效率如何，经济吗。
想到一个办法，第一次，前面随意，最后一层用一个神经元，不激活，用0做分界，分成2类，通过修改最后一层的bias（就是wx+b的b），分出来的2类的比例应该是有一个变化的，中间应该有一段不怎么变的，或者变得很快的，卡在这个中间。然后把最后一层变成2个神经元，第二个神经元的w和b是第一个的-1倍，这样就变成了一个多分类的结构了，最后加上softmax即可。也可以还是只用一个神经元，分成2类，然后在分出来的2个类里面，再分。最终要得到一个大的神经网络，可以用分出来的n个类，从头训练一个新的，也可以手动写w和b，用relu当成if，应该也可以。
（草稿结束）

2023 4月 7，给完全无标注的数据做分类，而且是自适应分类数，可能已经可行了。
可能可以对无标注数据做分类。我记得以前有一个相似性学习，不过我不知道那个是怎么做的。
我的思路是，假设一堆数据没标注，属于大概100个类，先找一个很小的神经网络，绝对不够用的那种，很小，随便初始化一下，最后一层是一个神经元，wx+b，出来可以sigmoid也可以不，没有什么区别。这样可以把样本分成2类，通过手动修改最后一层的b，可以让分出来的2个类的占比不同，于是可以得到 第一类的占比 = f（b），以这个占比为10%和90%为界（这个是超参，请随便调整，可以更往两头靠，比如5%和95%这样）计算中间的 d第一类的占比/db，也就是修改b的时候，b划过某些区域会导致分类结果迅速变化，我觉得这种地方应该就是不应该分的地方，因为迅速变化是一些相似的样本聚集在一起，b划过他们，引起的。如此一来，d第一类的占比/db 小的地方应该就是可以安全的分开的地方。但是有一个问题，就是在最两头，通常样本会更少，这种地方会被误认为是应该分开的地方，我觉得有效的方法可能是，在 d第一类的占比/db 上乘一个系数，这个系数要两边大，中间小，我觉得可能用高斯函数会比较适合，就是正态分布的那啥，方差的话，可能要和前面说的10%，90%的界限做一些对齐，不过就算对齐得不是很科学，应该也不怎么影响。然后找到这个修正后的结果的最小值，以此为分界依据，用对应的b更新原来神经往后最后一层的那个b，然后再分一次，这一次在最后一层后面加上sigmoid，然后只看神经网络最有信心的部分。这个最有信心的具体做法是，只考虑结果小于0.1或者大于0.9的部分，看看这一部分占样本总量的多少，调节这个分界，以“神经网络对50%的样本很有信心”为准，当然这个50%也是超参，请随意修改。反正就是小于0.xx，或者大于1-0.xx，为准，对称的。
然后拿这一部分来训练另外一个类似的神经网络。这儿不是很清楚，如果训练同样结构的神经网络，或者对同一个神经网络进行训练会如何。其实就算是训练同一个神经网络，应该多少也会带来一些变化，因为原来的神经网络对某些样本的预测是0.2，在新的训练里面，这些样本对应的标注应该是0，或者就算加噪音，也是-0.1到0.1之间，还是有区别的。当然如果直接用一个规模类似，但是结构不同的神经网络，比如把mlp换成transformer一类的，可能会更有用，原因在后面。
然后用经过训练的神经网络，再做预测，这次依然是在整个数据集上预测，依然和前面的步骤一样。经过几次之后，应该能找到一个相对比较靠谱的分类依据。然后这个时候就可以尝试增加神经网络的规模了。

前面这一段是把一些样本一分为二，在分开的2个类里面还可以分别再进行细分。可以把一个样本无限的分下去。这是分开，后面要讲合并。
假设现在已经分了3轮了，总共有8个类，可以训练一个神经网络来尝试从总的样本里面直接分这8个类，就是一个很传统的多分类任务。这种神经网络的最后是一个8个神经元的层，没有激活，出来以后是做一次softmax。如果取softmax的输入，也就是最后一层的输出，为每一个样本的分类依据，那么可以用这个依据的相似性来判断这些分类有没有道理。具体做法是，用这个分类依据的前7个数字去预测最后一个数字，如果正确率过半（或者不用一半作为分界，超参，请随便调整），那么最后一个分类就和前面某一个分类有很大的相似性，这个时候可以用统计上的某种方法，比如相关性，看这个8号特征和前面的哪一个特征的相关性最大，找到了先放着，等后面的检查结束了，在数据集那个层面直接给合并上。刚刚是前7个特征去预测第8个特征，然后接下来就是前6个特征去预测第7个特征，如果符合什么标准，那么一样的，找相关性最大的。
前面这个步骤结束以后，之前是在数据集上直接做了分割，分成了8个部分，现在知道了其中有一些相似性是非常大的，于是把他们重新合并起来，如果8号和某一个合并了，那么总共就只会有7个子集，下次再尝试分割的时候就只有7个单独的尝试。

最后还有一点细节。
如果分出来超过30个类了，可以考虑只对样本数最多的1/3的分类再做细分。但是合并阶段是要考虑全部的分类的。如果连续多少轮次都没有变化，那么整个分类就结束。

好像可以做个简单的文档了。







图片，视频一类的数据集，可以从文本描述里面来尝试扣物体间的位置关系，包括其他关系，从而大致的猜物体的位置，从而获得更大量的数据，但是不知道这个里面的噪音会不会过大。


图形上的data argmentation可以考虑一下不要那么严重







人工意识的健康问题？
人类的神经系统可以大致近似成一个diffusion（推导过程以后慢慢补）。如果让这个diffusion一直运行，是不是可以永生？
加上如果让多个diffusion对同一个vector上的不同部位分别进行更新，只要他们有重叠，甚至就是交叉给信息给对方，应该可以减小参数的量，而且实际效果可能类似大脑的不同结构之间的隔离，或者脊椎。
这个系统只要维持某些输出值在一个范围，当这些值超出这个范围的时候，以边界为目标进行训练（或者任何等效的方法），无论是回滚，还是训练之后继续运行，等它自己回到这个设计的范围，就可以了。整个nn就是为了维持这些值。完全的仿生学。。
如果一些事情被联系上了这些必须在某个范围内的值，是不是最终整个nn表现出来会有一个倾向性，类似兴趣，一类的。
是否真的有归因，还是只有抽象？抽象如果按diffusion来解释，就是一步做2步的事情？
如果把每一步的输出都保存下来，应该也用不了多少内存，比如他们是a1，a2，...，an，是否可以用a1作为输入，a3作为输出，训练这个diffusion，或者其他的，反正就是不是i到i+1，而是加超过1，比如i到i+3一类的，从而得到近道。这种近道应该可以拟合相关性，抽象，但是似乎无法拟合因果，不过我反正不觉得因果是一个真实存在的东西，好像也就无所谓了。
接下来，这个系统要怎么做生成？尤其效率还不错的生成？
包括这个系统看来好像是无法做很创新的事情的。

一个利用abstraction来进行学习的方法。
除了之前提到的，可以在t1和t3之间直接进行一次学习，从而尝试跳过t2。还有一种方法可能可以提供一个很好的辅助。
假设在两个形式之间要进行转换，假设形式是a和b，a1是a形式的一个内部表示，利用某种转换，得到了b1，形式是b。如果用b1再转换回a形式，多半得不到a1，新的结果记作a2，那么就会有一个差，就是a2-a1，这个时候，如果用（a1*2-a2）来作为开始，生成一个b形式的结果，就有可能得到本来想用a1来转换出来的结果，至少会比较接近，这个结果记作b2。如果确实比较接近，那么可以利用a1和b2作为训练。本来a1应该是对应b1的，但是b2更优，用这个方法可以找到b2，从而得到更优的训练数据。






最近的segment anything model(from meta)，可能还不够细。假设现在有一个足够细的这种分割模型，是否可以分割以后，对每一个物体做一些平移，变形一类的，从而在生成视频的时候得到一个连贯性。






nerf 的dream booth和lora的可能性？以及其他一切的类似的做法，包括基于llama2的？
